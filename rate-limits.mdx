---
title: 'Rate Limits'
description: 'Understanding API rate limits and best practices'
---

## Rate Limit Overview

The Blockworks Research API implements rate limiting to ensure fair usage and optimal performance for all users. Rate limits are applied per API key and vary based on your subscription plan.

## Rate Limit Tiers

| Plan | Requests per Hour | Requests per Day | Burst Limit |
|------|-------------------|--------------|-------------|
| **Starter** | 1,000 | 10,000 | 100/minute |
| **Professional** | 10,000 | 100,000 | 1,000/minute |
| **Enterprise** | Custom | Custom | Custom |

<Card
  title="Upgrade Your Plan"
  icon="arrow-up"
  href="https://www.blockworksresearch.com/#pricing"
>
  Need higher limits? Upgrade your plan for increased API access
</Card>

## Rate Limit Headers

Every API response includes rate limit information in the headers:

```http
X-RateLimit-Limit: 1000
X-RateLimit-Remaining: 999
X-RateLimit-Reset: 1640995200
X-RateLimit-Window: 3600
```

### Header Descriptions

| Header | Description |
|--------|-------------|
| `X-RateLimit-Limit` | Maximum requests allowed in the current window |
| `X-RateLimit-Remaining` | Number of requests remaining in current window |
| `X-RateLimit-Reset` | Unix timestamp when the rate limit resets |
| `X-RateLimit-Window` | Rate limit window duration in seconds |

## Rate Limit Exceeded

When you exceed your rate limit, you'll receive a `429 Too Many Requests` response:

```json
{
  "error": {
    "code": "RATE_LIMIT_EXCEEDED",
    "message": "Rate limit exceeded",
    "details": {
      "limit": 1000,
      "remaining": 0,
      "reset": 1640995200,
      "retry_after": 300
    }
  }
}
```

## Best Practices

### 1. Monitor Rate Limit Headers

Always check the rate limit headers in your responses:

<CodeGroup>

```typescript TypeScript
async function makeAPIRequest(url: string) {
  const response = await fetch(url, {
    headers: { 'x-api-key': 'YOUR_API_KEY' }
  });
  
  // Check rate limit headers
  const remaining = response.headers.get('X-RateLimit-Remaining');
  const reset = response.headers.get('X-RateLimit-Reset');
  
  console.log(`Remaining requests: ${remaining}`);
  console.log(`Reset time: ${new Date(parseInt(reset) * 1000)}`);
  
  if (response.status === 429) {
    const retryAfter = response.headers.get('Retry-After');
    console.log(`Rate limited. Retry after: ${retryAfter} seconds`);
    throw new Error('Rate limit exceeded');
  }
  
  return response.json();
}
```

```python Python
import requests
import time
from datetime import datetime

def make_api_request(url: str, api_key: str):
    headers = {'x-api-key': api_key}
    response = requests.get(url, headers=headers)
    
    # Check rate limit headers
    remaining = response.headers.get('X-RateLimit-Remaining')
    reset_time = response.headers.get('X-RateLimit-Reset')
    
    print(f"Remaining requests: {remaining}")
    if reset_time:
        reset_datetime = datetime.fromtimestamp(int(reset_time))
        print(f"Reset time: {reset_datetime}")
    
    if response.status_code == 429:
        retry_after = response.headers.get('Retry-After')
        print(f"Rate limited. Retry after: {retry_after} seconds")
        raise Exception('Rate limit exceeded')
    
    return response.json()
```

</CodeGroup>

### 2. Implement Exponential Backoff

Handle rate limit errors gracefully with exponential backoff:

<CodeGroup>

```typescript TypeScript
async function apiRequestWithBackoff(url: string, maxRetries: number = 3): Promise<any> {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      const response = await fetch(url, {
        headers: { 'x-api-key': 'YOUR_API_KEY' }
      });
      
      if (response.status === 429) {
        const retryAfter = parseInt(response.headers.get('Retry-After') || '60');
        const delay = Math.min(retryAfter * 1000, Math.pow(2, attempt) * 1000);
        
        console.log(`Rate limited. Waiting ${delay}ms before retry ${attempt + 1}`);
        await new Promise(resolve => setTimeout(resolve, delay));
        continue;
      }
      
      if (response.ok) {
        return response.json();
      }
      
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    } catch (error) {
      if (attempt === maxRetries - 1) throw error;
      
      const delay = Math.pow(2, attempt) * 1000;
      console.log(`Attempt ${attempt + 1} failed. Retrying in ${delay}ms`);
      await new Promise(resolve => setTimeout(resolve, delay));
    }
  }
}
```

```python Python
import requests
import time
import random

def api_request_with_backoff(url: str, api_key: str, max_retries: int = 3):
    headers = {'x-api-key': api_key}
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers)
            
            if response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', 60))
                delay = min(retry_after, 2 ** attempt)
                
                # Add jitter to prevent thundering herd
                jitter = random.uniform(0.1, 0.3) * delay
                total_delay = delay + jitter
                
                print(f"Rate limited. Waiting {total_delay:.1f}s before retry {attempt + 1}")
                time.sleep(total_delay)
                continue
            
            if response.status_code == 200:
                return response.json()
            
            response.raise_for_status()
            
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                raise e
            
            delay = 2 ** attempt
            print(f"Attempt {attempt + 1} failed. Retrying in {delay}s")
            time.sleep(delay)
    
    raise Exception(f"Failed after {max_retries} attempts")
```

</CodeGroup>

### 3. Batch Requests Efficiently

Optimize your API usage by batching related requests:

<CodeGroup>

```typescript TypeScript
class BatchProcessor {
  private apiKey: string;
  private requestQueue: Array<{ url: string; resolve: Function; reject: Function }> = [];
  private batchSize = 10;
  private batchDelay = 1000; // 1 second between batches

  constructor(apiKey: string) {
    this.apiKey = apiKey;
  }

  async queueRequest(url: string): Promise<any> {
    return new Promise((resolve, reject) => {
      this.requestQueue.push({ url, resolve, reject });
      this.processBatch();
    });
  }

  private async processBatch() {
    if (this.requestQueue.length === 0) return;

    const batch = this.requestQueue.splice(0, this.batchSize);
    
    try {
      const promises = batch.map(({ url }) =>
        fetch(url, { headers: { 'x-api-key': this.apiKey } })
          .then(response => response.json())
      );

      const results = await Promise.all(promises);
      
      batch.forEach(({ resolve }, index) => {
        resolve(results[index]);
      });

      // Wait before processing next batch
      if (this.requestQueue.length > 0) {
        setTimeout(() => this.processBatch(), this.batchDelay);
      }
    } catch (error) {
      batch.forEach(({ reject }) => reject(error));
    }
  }
}

// Usage
const batcher = new BatchProcessor('YOUR_API_KEY');
const assets = ['bitcoin', 'ethereum', 'solana'];

const results = await Promise.all(
  assets.map(asset => 
    batcher.queueRequest(`https://rest.staging.blockworksresearch.com/v3/assets/${asset}`)
  )
);
```

```python Python
import asyncio
import aiohttp
from typing import List, Dict, Any

class BatchProcessor:
    def __init__(self, api_key: str, batch_size: int = 10, delay: float = 1.0):
        self.api_key = api_key
        self.batch_size = batch_size
        self.delay = delay
        self.headers = {'x-api-key': api_key}
    
    async def process_batch(self, urls: List[str]) -> List[Dict[str, Any]]:
        """Process a batch of URLs with rate limiting"""
        results = []
        
        for i in range(0, len(urls), self.batch_size):
            batch = urls[i:i + self.batch_size]
            
            async with aiohttp.ClientSession() as session:
                tasks = [
                    self.fetch_with_retry(session, url)
                    for url in batch
                ]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                results.extend(batch_results)
            
            # Wait between batches to respect rate limits
            if i + self.batch_size < len(urls):
                await asyncio.sleep(self.delay)
        
        return results
    
    async def fetch_with_retry(self, session: aiohttp.ClientSession, url: str, max_retries: int = 3):
        """Fetch URL with retry logic"""
        for attempt in range(max_retries):
            try:
                async with session.get(url, headers=self.headers) as response:
                    if response.status == 429:
                        retry_after = int(response.headers.get('Retry-After', 60))
                        await asyncio.sleep(retry_after)
                        continue
                    
                    response.raise_for_status()
                    return await response.json()
                    
            except Exception as e:
                if attempt == max_retries - 1:
                    return {'error': str(e), 'url': url}
                await asyncio.sleep(2 ** attempt)
        
        return {'error': 'Max retries exceeded', 'url': url}

# Usage
async def main():
    processor = BatchProcessor('YOUR_API_KEY')
    urls = [
        'https://rest.staging.blockworksresearch.com/v3/assets/bitcoin',
        'https://rest.staging.blockworksresearch.com/v3/assets/ethereum',
        'https://rest.staging.blockworksresearch.com/v3/assets/solana'
    ]
    
    results = await processor.process_batch(urls)
    for result in results:
        print(result)

# asyncio.run(main())
```

</CodeGroup>

### 4. Cache Responses

Implement caching to reduce API calls:

<CodeGroup>

```typescript TypeScript
class APICache {
  private cache = new Map<string, { data: any; timestamp: number; ttl: number }>();

  get(key: string): any | null {
    const item = this.cache.get(key);
    if (!item) return null;
    
    if (Date.now() > item.timestamp + item.ttl) {
      this.cache.delete(key);
      return null;
    }
    
    return item.data;
  }

  set(key: string, data: any, ttl: number = 300000): void { // 5 minutes default
    this.cache.set(key, {
      data,
      timestamp: Date.now(),
      ttl
    });
  }

  async fetchWithCache(url: string, ttl?: number): Promise<any> {
    const cached = this.get(url);
    if (cached) {
      console.log('Cache hit for:', url);
      return cached;
    }

    console.log('Cache miss for:', url);
    const response = await fetch(url, {
      headers: { 'x-api-key': 'YOUR_API_KEY' }
    });
    
    const data = await response.json();
    this.set(url, data, ttl);
    
    return data;
  }
}

// Usage
const cache = new APICache();
const data = await cache.fetchWithCache(
  'https://rest.staging.blockworksresearch.com/v3/assets/bitcoin',
  300000 // Cache for 5 minutes
);
```

```python Python
import time
import requests
from typing import Dict, Any, Optional

class APICache:
    def __init__(self):
        self.cache: Dict[str, Dict[str, Any]] = {}
    
    def get(self, key: str) -> Optional[Any]:
        """Get item from cache if still valid"""
        if key not in self.cache:
            return None
        
        item = self.cache[key]
        if time.time() > item['timestamp'] + item['ttl']:
            del self.cache[key]
            return None
        
        return item['data']
    
    def set(self, key: str, data: Any, ttl: int = 300) -> None:
        """Set item in cache with TTL"""
        self.cache[key] = {
            'data': data,
            'timestamp': time.time(),
            'ttl': ttl
        }
    
    def fetch_with_cache(self, url: str, api_key: str, ttl: int = 300) -> Any:
        """Fetch URL with caching"""
        cached = self.get(url)
        if cached:
            print(f"Cache hit for: {url}")
            return cached
        
        print(f"Cache miss for: {url}")
        headers = {'x-api-key': api_key}
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        
        data = response.json()
        self.set(url, data, ttl)
        
        return data

# Usage
cache = APICache()
data = cache.fetch_with_cache(
    'https://rest.staging.blockworksresearch.com/v3/assets/bitcoin',
    'YOUR_API_KEY',
    300  # Cache for 5 minutes
)
```

</CodeGroup>

## Monitoring Your Usage

Keep track of your API usage to avoid hitting limits:

### Usage Tracking

```typescript
class UsageTracker {
  private requests: number[] = [];
  private window = 3600000; // 1 hour in milliseconds

  recordRequest(): void {
    const now = Date.now();
    this.requests.push(now);
    
    // Remove requests outside the window
    this.requests = this.requests.filter(time => now - time < this.window);
  }

  getCurrentUsage(): { count: number; percentage: number } {
    const count = this.requests.length;
    const limit = 1000; // Your hourly limit
    
    return {
      count,
      percentage: (count / limit) * 100
    };
  }
}
```

## Getting Help

If you're consistently hitting rate limits:

<CardGroup cols={2}>
  <Card
    title="Upgrade Your Plan"
    icon="arrow-up"
    href="https://www.blockworksresearch.com/#pricing"
  >
    Get higher rate limits with a Professional or Enterprise plan
  </Card>
  <Card
    title="Contact Support"
    icon="envelope"
    href="mailto:support@blockworksresearch.com"
  >
    Contact us for custom rate limit discussions
  </Card>
</CardGroup>

## Next Steps

<CardGroup cols={2}>
  <Card
    title="API Reference"
    icon="book"
    href="/api-reference/introduction"
  >
    Learn more about API endpoints and usage
  </Card>
  <Card
    title="Best Practices"
    icon="lightbulb"
    href="/examples/typescript"
  >
    See implementation examples with proper rate limiting
  </Card>
</CardGroup>